{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11ce977",
   "metadata": {},
   "source": [
    "# Reforge ‚Äî Benchmark v2: Experiment Configuration & Execution\n",
    "\n",
    "This notebook generates, validates, budgets, and executes the full benchmark experiment matrix.\n",
    "\n",
    "### Design\n",
    "- **13 models** √ó **3 tiers** (GOLD/SILVER/BRONZE) √ó **4 opt levels** (O0‚ÄìO3) √ó **3 context levels** (L0/L1/L2) = **468 experiments**\n",
    "- Context ablation: L0 (code-only) ‚Üí L1 (+calls) ‚Üí L2 (+calls+CFG+vars)\n",
    "- All v2 prompt templates, no function limit\n",
    "- Budget ceiling: **$300**\n",
    "\n",
    "### Non-Negotiable Constraint\n",
    "> **LLM input MUST contain ONLY Ghidra-derived artefacts.**  \n",
    "> Ground truth (DWARF / source / join metadata) is used ONLY post-hoc for scoring.  \n",
    "> The `/llm/functions` endpoint enforces this by construction.\n",
    "\n",
    "### Prerequisites\n",
    "- Docker stack running: `docker compose up -d` in `reforge/docker/`\n",
    "- Services: `api` (port 8080), `redis`, `postgres`\n",
    "- `OPENROUTER_API_KEY` set in `docker/.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac77949",
   "metadata": {},
   "source": [
    "## ¬ß1 ‚Äî Setup & Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API: {'status': 'healthy', 'service': 'reforge-api', 'version': '0.1.0'}\n",
      "Key: ...e0ea6829\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json, time, importlib, textwrap\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import requests\n",
    "\n",
    "# Ensure reforge root is on sys.path\n",
    "REFORGE_ROOT = Path(\".\").resolve().parent\n",
    "if str(REFORGE_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REFORGE_ROOT))\n",
    "\n",
    "API = \"http://localhost:8080\"\n",
    "OPENROUTER_KEY = os.environ.get(\n",
    "    \"OPENROUTER_API_KEY\",\n",
    "    \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    ")\n",
    "\n",
    "def api(path, **kw):  return requests.get(f\"{API}{path}\", **kw).json()\n",
    "def post(url, body):  return requests.post(url, json=body).json()\n",
    "\n",
    "health = api(\"/health\")\n",
    "print(f\"API: {health}\")\n",
    "print(f\"Key: ...{OPENROUTER_KEY[-8:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b146ee",
   "metadata": {},
   "source": [
    "## ¬ß2 ‚Äî Review Legacy Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3edde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 5\n",
      "Legacy (pilot):    0\n",
      "Active:            5\n",
      "\n",
      "\n",
      "  ‚úÖ  exp01_funcnaming_gpt4omini_gold_O0                  openai/gpt-4o-mini                   L0\n",
      "  ‚úÖ  exp02_funcnaming_gpt4o_gold_O0                      openai/gpt-4o                        L0\n",
      "  ‚úÖ  exp03_funcnaming_claude_gold_O0                     anthropic/claude-3.5-sonnet          L0\n",
      "  ‚úÖ  exp04_funcnaming_gpt4omini_gold_O2                  openai/gpt-4o-mini                   L0\n",
      "  üìù  exp05_funcnaming_gpt4omini_silver_O0                openai/gpt-4o-mini                   L0\n"
     ]
    }
   ],
   "source": [
    "experiments = api(\"/data/experiments\")\n",
    "\n",
    "legacy = [e for e in experiments if e.get('status') == 'legacy']\n",
    "active = [e for e in experiments if e.get('status') != 'legacy']\n",
    "\n",
    "print(f\"Total experiments: {len(experiments)}\")\n",
    "print(f\"Legacy (pilot):    {len(legacy)}\")\n",
    "print(f\"Active:            {len(active)}\")\n",
    "print()\n",
    "for e in legacy:\n",
    "    print(f\"  üóÑÔ∏è  {e['id']:50s}  {e['model']}\")\n",
    "print()\n",
    "for e in active[:20]:\n",
    "    status_icon = {'ready': '‚úÖ', 'draft': 'üìù', 'running': '‚è≥', 'completed': '‚úîÔ∏è'}.get(e['status'], '?')\n",
    "    print(f\"  {status_icon}  {e['id']:50s}  {e['model']:35s}  {e.get('context_level', 'L0')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38678e",
   "metadata": {},
   "source": [
    "## ¬ß3 ‚Äî Build the Benchmark Matrix\n",
    "\n",
    "Generate all experiment configs programmatically.  \n",
    "Edit the slices below to control scope ‚Äî the full matrix is 468 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd52007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark matrix: 1 experiments\n",
      "  Models:  1\n",
      "  Tiers:   ['GOLD']\n",
      "  Opts:    ['O3']\n",
      "  Context: ['L2']\n",
      "\n",
      "  bench_gpt4o-mini_gold_O3_L2                             ‚Üí openai/gpt-4o-mini                   L2\n",
      "\n",
      "Registering experiments with API server...\n",
      "  ‚úÖ Registered 1 experiments (1 new, 0 updated)\n"
     ]
    }
   ],
   "source": [
    "from data.experiments import (\n",
    "    build_benchmark_matrix,\n",
    "    estimate_benchmark_cost,\n",
    "    BENCHMARK_MODELS,\n",
    "    BENCHMARK_TIERS,\n",
    "    BENCHMARK_OPTS,\n",
    "    BENCHMARK_CONTEXT_LEVELS,\n",
    "    REGISTRY,\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ Customise the slice here ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# For a test run, uncomment and narrow:\n",
    "selected_models = {\"gpt4o-mini\": BENCHMARK_MODELS[\"gpt4o-mini\"]}\n",
    "selected_tiers  = [\"GOLD\"]\n",
    "selected_opts   = [\"O3\"]\n",
    "selected_ctx    = [\"L2\"]\n",
    "#\n",
    "# Full benchmark:\n",
    "# selected_models = BENCHMARK_MODELS\n",
    "# selected_tiers  = BENCHMARK_TIERS\n",
    "# selected_opts   = BENCHMARK_OPTS\n",
    "# selected_ctx    = BENCHMARK_CONTEXT_LEVELS\n",
    "\n",
    "matrix = build_benchmark_matrix(\n",
    "    models=selected_models,\n",
    "    tiers=selected_tiers,\n",
    "    opts=selected_opts,\n",
    "    context_levels=selected_ctx,\n",
    "    register=True,\n",
    ")\n",
    "\n",
    "print(f\"Benchmark matrix: {len(matrix)} experiments\")\n",
    "print(f\"  Models:  {len(selected_models)}\")\n",
    "print(f\"  Tiers:   {selected_tiers}\")\n",
    "print(f\"  Opts:    {selected_opts}\")\n",
    "print(f\"  Context: {selected_ctx}\")\n",
    "print()\n",
    "# Show first 10\n",
    "for cfg in matrix[:10]:\n",
    "    print(f\"  {cfg.id:55s} ‚Üí {cfg.model:35s}  {cfg.context_level}\")\n",
    "if len(matrix) > 10:\n",
    "    print(f\"  ... and {len(matrix) - 10} more\")\n",
    "\n",
    "# ‚îÄ‚îÄ Push matrix to the API server so /data/experiments/{id} works ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print()\n",
    "print(\"Registering experiments with API server...\")\n",
    "resp = requests.post(\n",
    "    f\"{API}/data/experiments/bulk\",\n",
    "    json=[cfg.model_dump() for cfg in matrix],\n",
    ")\n",
    "if resp.status_code in (200, 201):\n",
    "    reg = resp.json()\n",
    "    print(f\"  ‚úÖ Registered {reg['registered']} experiments ({reg['created']} new, {reg['updated']} updated)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Registration failed: {resp.status_code} ‚Äî {resp.text}\")\n",
    "    print(\"  Dry-run / execution cells will fail with 404 until experiments are registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27661d48",
   "metadata": {},
   "source": [
    "## ¬ß4 ‚Äî Budget Estimation\n",
    "\n",
    "Estimate cost before committing.  Uses rough OpenRouter pricing tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb887305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions per (tier, opt):\n",
      "  GOLD     O0 :  197 functions\n",
      "\n",
      "Average per experiment: ~197 functions\n",
      "\n",
      "Total experiments:   1\n",
      "Total LLM calls:    197\n",
      "Total input tokens:  157,600\n",
      "Estimated cost:      $0.02\n",
      "\n",
      "‚úÖ Within $300 budget ‚Äî 299.98 remaining\n",
      "\n",
      "Per-model cost:\n",
      "  $    0.02  openai/gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Count how many functions we actually have per tier/opt\n",
    "func_counts = {}\n",
    "for tier in selected_tiers:\n",
    "    for opt_lvl in selected_opts:\n",
    "        try:\n",
    "            fns = api(\"/llm/functions\", params={\"opt\": opt_lvl, \"tier\": tier, \"limit\": 5000})\n",
    "            func_counts[(tier, opt_lvl)] = len(fns)\n",
    "        except Exception:\n",
    "            func_counts[(tier, opt_lvl)] = 0\n",
    "\n",
    "total_functions_per_exp = sum(func_counts.values()) // (len(selected_tiers) * len(selected_opts))\n",
    "print(\"Functions per (tier, opt):\")\n",
    "for (t, o), n in sorted(func_counts.items()):\n",
    "    print(f\"  {t:8s} {o:3s}: {n:4d} functions\")\n",
    "print(f\"\\nAverage per experiment: ~{total_functions_per_exp} functions\")\n",
    "print()\n",
    "\n",
    "# Estimate cost\n",
    "est = estimate_benchmark_cost(\n",
    "    matrix,\n",
    "    avg_prompt_tokens=800,\n",
    "    avg_completion_tokens=20,\n",
    "    functions_per_experiment=max(total_functions_per_exp, 50),\n",
    ")\n",
    "\n",
    "print(f\"Total experiments:   {est['total_experiments']}\")\n",
    "print(f\"Total LLM calls:    {est['total_calls']:,}\")\n",
    "print(f\"Total input tokens:  {est['total_input_tokens']:,}\")\n",
    "print(f\"Estimated cost:      ${est['estimated_cost_usd']:.2f}\")\n",
    "print()\n",
    "\n",
    "BUDGET = 300.0\n",
    "if est['estimated_cost_usd'] > BUDGET:\n",
    "    print(f\"‚ö†Ô∏è  OVER BUDGET (${BUDGET:.0f}). Narrow the matrix or reduce models.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Within ${BUDGET:.0f} budget ‚Äî {BUDGET - est['estimated_cost_usd']:.2f} remaining\")\n",
    "    \n",
    "# Per-model breakdown\n",
    "model_costs = {}\n",
    "for item in est['breakdown']:\n",
    "    m = item['model']\n",
    "    model_costs[m] = model_costs.get(m, 0) + item['est_cost_usd']\n",
    "print(\"\\nPer-model cost:\")\n",
    "for m, c in sorted(model_costs.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  ${c:8.2f}  {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490926a",
   "metadata": {},
   "source": [
    "## ¬ß5 ‚Äî Dry-Run Validation\n",
    "\n",
    "Pick one experiment per context level. Validate prompt rendering and API round-trip  \n",
    "**without** making real LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74fc458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: cu0x0:die0x12d\n",
      "  ghidra_name:  FUN_004011fa\n",
      "  test_case:    t01_crossfile_calls\n",
      "  loc:          17\n",
      "  cyclomatic:   None\n",
      "  bb_count:     None\n",
      "\n",
      "================================================================================\n",
      "FULL RENDERED PROMPT (L2)\n",
      "================================================================================\n",
      "You are an expert reverse engineer analyzing decompiled binary code.\n",
      "\n",
      "A function has been decompiled from a stripped binary using Ghidra. The original\n",
      "symbol names have been removed by the strip tool. Your task is to analyze the\n",
      "decompiled C code along with its structural metadata, then suggest a meaningful,\n",
      "descriptive function name that reflects what the function does.\n",
      "\n",
      "Guidelines:\n",
      "- Use snake_case naming convention\n",
      "- Be specific but concise ‚Äî prefer 2-4 words\n",
      "- Focus on the function's PURPOSE, not its implementation details\n",
      "- Use call relationships and variable information to understand context\n",
      "- Consider control-flow complexity when reasoning about function role\n",
      "- If the function is a standard library wrapper, name it accordingly\n",
      "- If you cannot determine the purpose, use a descriptive structural name\n",
      "\n",
      "Respond with ONLY the suggested function name, nothing else. No explanation,\n",
      "no punctuation, no quotes ‚Äî just the name in snake_case.\n",
      "\n",
      "## Decompiled Code\n",
      "\n",
      "```c\n",
      "\n",
      "int FUN_004011fa(int *param_1,int param_2)\n",
      "\n",
      "{\n",
      "  int local_10;\n",
      "  int local_c;\n",
      "  \n",
      "  if (param_2 < 1) {\n",
      "    local_c = 0;\n",
      "  }\n",
      "  else {\n",
      "    local_c = *param_1;\n",
      "    for (local_10 = 1; local_10 < param_2; local_10 = local_10 + 1) {\n",
      "      if (param_1[local_10] < local_c) {\n",
      "        local_c = param_1[local_10];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  return local_c;\n",
      "}\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## Call Relationships\n",
      "\n",
      "(no outgoing calls)\n",
      "\n",
      "## Control Flow Summary\n",
      "\n",
      "Control-flow summary:\n",
      "  - Basic blocks: 9\n",
      "  - Edges: 11\n",
      "  - Cyclomatic complexity: 4\n",
      "  - Has indirect jumps: no\n",
      "  - CFG completeness: HIGH\n",
      "\n",
      "## Local Variables\n",
      "\n",
      "Variables:\n",
      "  - local_10: int (LOCAL, STACK)\n",
      "  - local_c: int (LOCAL, STACK)\n",
      "  - param_2: int (PARAM, REGISTER)\n",
      "  - param_1: int * (PARAM, REGISTER)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 1704 chars  (~426 tokens)\n"
     ]
    }
   ],
   "source": [
    "## Preview: render one full prompt at L2 (code + calls + CFG + variables)\n",
    "\n",
    "from workers.llm.prompt import load_template, render_prompt\n",
    "\n",
    "# Fetch one L2 function from the API\n",
    "sample = api(\"/llm/functions\", params={\n",
    "    \"opt\": \"O0\",\n",
    "    \"tier\": \"GOLD\",\n",
    "    \"context_level\": \"L2\",\n",
    "    \"limit\": 1,\n",
    "})\n",
    "\n",
    "if not sample:\n",
    "    print(\"‚ö†Ô∏è  No functions returned ‚Äî is the data loaded?\")\n",
    "else:\n",
    "    fn = sample[0]\n",
    "    print(f\"Function: {fn['dwarf_function_id']}\")\n",
    "    print(f\"  ghidra_name:  {fn.get('ghidra_name')}\")\n",
    "    print(f\"  test_case:    {fn.get('test_case')}\")\n",
    "    print(f\"  loc:          {fn.get('loc_decompiled')}\")\n",
    "    print(f\"  cyclomatic:   {fn.get('cyclomatic')}\")\n",
    "    print(f\"  bb_count:     {fn.get('bb_count')}\")\n",
    "    print()\n",
    "\n",
    "    # Load the L2 template and render\n",
    "    template = load_template(\"function_naming_v2_L2\")\n",
    "    prompt = render_prompt(\n",
    "        template,\n",
    "        fn.get(\"c_raw\", \"\"),\n",
    "        calls=fn.get(\"calls_text\"),\n",
    "        cfg_summary=fn.get(\"cfg_text\"),\n",
    "        variables=fn.get(\"variables_text\"),\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FULL RENDERED PROMPT (L2)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nPrompt length: {len(prompt)} chars  (~{len(prompt)//4} tokens)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23734c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dry-run targets: ['bench_gpt4o-mini_gold_O3_L2']\n",
      "\n",
      "\n",
      "============================================================\n",
      "DRY RUN: bench_gpt4o-mini_gold_O3_L2\n",
      "============================================================\n",
      "  Total: 85, New: 85, Errors: 0\n",
      "  ‚úÖ PASS\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from workers.llm.runner import run_experiment\n",
    "\n",
    "# Pick one cheap-model experiment per context level for validation\n",
    "dry_run_ids = [\n",
    "    cfg.id for cfg in matrix\n",
    "    if \"gpt4o-mini\" in cfg.id and \"gold\" in cfg.id and \"O3\" in cfg.id\n",
    "][:3]  # Should be L0, L1, L2\n",
    "\n",
    "print(f\"Dry-run targets: {dry_run_ids}\")\n",
    "print()\n",
    "\n",
    "for exp_id in dry_run_ids:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DRY RUN: {exp_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    summary = await run_experiment(\n",
    "        exp_id,\n",
    "        api_base=API,\n",
    "        dry_run=True,\n",
    "    )\n",
    "    print(f\"  Total: {summary['total']}, New: {summary['new']}, Errors: {summary['errors']}\")\n",
    "    print(f\"  {'‚úÖ PASS' if summary['errors'] == 0 else '‚ùå FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b569521",
   "metadata": {},
   "source": [
    "## ¬ß6 ‚Äî Execute Experiments\n",
    "\n",
    "### Execution Strategy\n",
    "- **Phase 1**: Cheap models first (gpt-4o-mini, llama, deepseek) ‚Äî validate pipeline\n",
    "- **Phase 2**: Mid-tier models (gpt-4o, claude-3.5-sonnet)\n",
    "- **Phase 3**: Premium models (gpt-5.1, claude-opus-4.6, codex-max)\n",
    "\n",
    "Each phase can be run independently. Results are idempotent (resume support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aa67e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase experiments: 1\n",
      "  bench_gpt4o-mini_gold_O3_L2\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Phase selector ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Uncomment ONE phase at a time, or set custom filter.\n",
    "\n",
    "CHEAP_MODELS = {\"gpt4o-mini\", \"llama31-70b\"} #, \"deepseek-coder2\", \"deepseek-v32\", \"deepseek-r1\", \"qwen3-coder\"\n",
    "MID_MODELS   = {\"gpt4o\", \"claude35sonnet\", \"claude-sonnet45\", \"gemini3-pro\"}\n",
    "PREMIUM_MODELS = {\"gpt51\", \"gpt51-codex-max\", \"claude-opus46\"}\n",
    "\n",
    "# Phase 1: cheap\n",
    "phase_filter = CHEAP_MODELS\n",
    "# Phase 2: mid\n",
    "# phase_filter = MID_MODELS\n",
    "# Phase 3: premium\n",
    "# phase_filter = PREMIUM_MODELS\n",
    "# All at once (‚ö†Ô∏è expensive):\n",
    "# phase_filter = CHEAP_MODELS | MID_MODELS | PREMIUM_MODELS\n",
    "\n",
    "phase_experiments = [\n",
    "    cfg for cfg in matrix\n",
    "    if any(label in cfg.id for label in phase_filter)\n",
    "]\n",
    "print(f\"Phase experiments: {len(phase_experiments)}\")\n",
    "for cfg in phase_experiments[:5]:\n",
    "    print(f\"  {cfg.id}\")\n",
    "if len(phase_experiments) > 5:\n",
    "    print(f\"  ... and {len(phase_experiments)-5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cade95b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/1] bench_gpt4o-mini_gold_O3_L2\n",
      "  Model: openai/gpt-4o-mini  Tier: GOLD  Opt: O3  Ctx: L2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (openai/gpt-4o-mini): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:10<00:00,  8.01fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Completed: 85, New: 85, Errors: 0\n",
      "\n",
      "============================================================\n",
      "Phase complete: 1 experiments, 0 total errors\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Execute the selected phase ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# This cell runs all experiments in the phase sequentially.\n",
    "# Each experiment uses internal async concurrency (5 parallel LLM calls).\n",
    "\n",
    "summaries = []\n",
    "errors_total = 0\n",
    "\n",
    "for i, cfg in enumerate(phase_experiments, 1):\n",
    "    print(f\"\\n[{i}/{len(phase_experiments)}] {cfg.id}\")\n",
    "    print(f\"  Model: {cfg.model}  Tier: {cfg.tier}  Opt: {cfg.opt}  Ctx: {cfg.context_level}\")\n",
    "    \n",
    "    try:\n",
    "        summary = await run_experiment(\n",
    "            cfg.id,\n",
    "            api_base=API,\n",
    "            openrouter_key=OPENROUTER_KEY,\n",
    "            concurrency=5,\n",
    "        )\n",
    "        summaries.append(summary)\n",
    "        \n",
    "        new = summary.get('new', 0)\n",
    "        errs = summary.get('errors', 0)\n",
    "        errors_total += errs\n",
    "        status = '‚úÖ' if errs == 0 else '‚ö†Ô∏è'\n",
    "        print(f\"  {status} Completed: {summary.get('completed', 0)}, New: {new}, Errors: {errs}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  ‚ùå FAILED: {exc}\")\n",
    "        errors_total += 1\n",
    "        summaries.append({\"experiment_id\": cfg.id, \"error\": str(exc)})\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Phase complete: {len(summaries)} experiments, {errors_total} total errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045a4fa",
   "metadata": {},
   "source": [
    "## ¬ß7 ‚Äî Score All Results\n",
    "\n",
    "Trigger the scorer for all experiments that have results but haven't been scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "436865e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ bench_gpt4o-mini_gold_O0_L2: 196 scored\n",
      "  ‚úÖ bench_gpt4o-mini_gold_O1_L2: 115 scored\n",
      "  ‚úÖ bench_gpt4o-mini_gold_O2_L2: 92 scored\n",
      "  ‚úÖ bench_gpt4o-mini_gold_O3_L2: 85 scored\n",
      "\n",
      "Scored: 4, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Fetch all experiments and trigger scoring for those with results\n",
    "all_exps = api(\"/data/experiments\")\n",
    "benchmark_exps = [e for e in all_exps if 'benchmark-v2' in e.get('tags', [])]\n",
    "\n",
    "scored = 0\n",
    "score_errors = 0\n",
    "for e in benchmark_exps:\n",
    "    exp_id = e['id']\n",
    "    try:\n",
    "        # Check if results exist\n",
    "        results = api(f\"/results/{exp_id}\")\n",
    "        if not results:\n",
    "            continue\n",
    "        \n",
    "        # Trigger scoring\n",
    "        resp = requests.post(f\"{API}/results/{exp_id}/score\")\n",
    "        if resp.status_code == 200:\n",
    "            score_data = resp.json()\n",
    "            n = score_data.get('scored', 0)\n",
    "            print(f\"  ‚úÖ {exp_id}: {n} scored\")\n",
    "            scored += 1\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  {exp_id}: {resp.status_code}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  ‚ùå {exp_id}: {exc}\")\n",
    "        score_errors += 1\n",
    "\n",
    "print(f\"\\nScored: {scored}, Errors: {score_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2fe27",
   "metadata": {},
   "source": [
    "## ¬ß8 ‚Äî Quick Progress Overview\n",
    "\n",
    "Show how many experiments have results and their scoring status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark progress (1 experiments):\n",
      "  No results yet: 0\n",
      "  Has results:    1\n",
      "  Scored:         0\n",
      "\n",
      "Per-model progress:\n",
      "  openai/gpt-4o-mini                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   1/  1 (100%)\n"
     ]
    }
   ],
   "source": [
    "all_exps = api(\"/data/experiments\")\n",
    "benchmark_exps = [e for e in all_exps if 'benchmark-v2' in e.get('tags', [])]\n",
    "\n",
    "progress = {\"no_results\": 0, \"has_results\": 0, \"scored\": 0}\n",
    "model_progress = {}\n",
    "\n",
    "for e in benchmark_exps:\n",
    "    exp_id = e['id']\n",
    "    model = e['model']\n",
    "    if model not in model_progress:\n",
    "        model_progress[model] = {\"total\": 0, \"done\": 0}\n",
    "    model_progress[model][\"total\"] += 1\n",
    "    \n",
    "    try:\n",
    "        results = api(f\"/results/{exp_id}\")\n",
    "        if results:\n",
    "            progress[\"has_results\"] += 1\n",
    "            model_progress[model][\"done\"] += 1\n",
    "            # Check if scored\n",
    "            try:\n",
    "                resp = api(f\"/results/{exp_id}/scores\")\n",
    "                rows = resp.get(\"rows\", []) if isinstance(resp, dict) else resp\n",
    "                if rows and any(s.get('token_f1') is not None for s in rows):\n",
    "                    progress[\"scored\"] += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            progress[\"no_results\"] += 1\n",
    "    except Exception:\n",
    "        progress[\"no_results\"] += 1\n",
    "\n",
    "print(f\"Benchmark progress ({len(benchmark_exps)} experiments):\")\n",
    "print(f\"  No results yet: {progress['no_results']}\")\n",
    "print(f\"  Has results:    {progress['has_results']}\")\n",
    "print(f\"  Scored:         {progress['scored']}\")\n",
    "print()\n",
    "print(\"Per-model progress:\")\n",
    "for m in sorted(model_progress.keys()):\n",
    "    p = model_progress[m]\n",
    "    pct = (p['done'] / p['total'] * 100) if p['total'] else 0\n",
    "    bar = '‚ñà' * int(pct // 5) + '‚ñë' * (20 - int(pct // 5))\n",
    "    print(f\"  {m:40s} {bar} {p['done']:3d}/{p['total']:3d} ({pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef547c2",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference\n",
    "\n",
    "### CLI execution (alternative to notebook)\n",
    "```bash\n",
    "# Single experiment\n",
    "cd reforge\n",
    "python -m workers.llm.runner --experiment bench_gpt4o-mini_gold_O0_L0 --api-base http://localhost:8080 --concurrency 5\n",
    "\n",
    "# Dry run\n",
    "python -m workers.llm.runner --experiment bench_gpt4o-mini_gold_O0_L0 --api-base http://localhost:8080 --dry-run\n",
    "\n",
    "# Batch (bash loop)\n",
    "for exp in bench_gpt4o-mini_gold_O0_L0 bench_gpt4o-mini_gold_O0_L1 bench_gpt4o-mini_gold_O0_L2; do\n",
    "    python -m workers.llm.runner --experiment $exp --api-base http://localhost:8080 -v\n",
    "done\n",
    "```\n",
    "\n",
    "### Analysis\n",
    "See `analysis.ipynb` for all figures, tables, and statistical tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
