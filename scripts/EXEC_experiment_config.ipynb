{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04255b69",
   "metadata": {},
   "source": [
    "# Reforge — Thesis Benchmark: Experiment Configuration & Execution\n",
    "\n",
    "This notebook generates, validates, budgets, and executes the thesis benchmark experiments.\n",
    "\n",
    "### Design\n",
    "- **3 models** × **3 tiers** (GOLD/SILVER/BRONZE) × **4 opt levels** (O0–O3) × **L2 context** × **top-3 JSON output** = **36 experiments**\n",
    "- Models: gpt-4o-mini (instant), deepseek-v3-0324 (coding), gpt-5.1 (thinking)\n",
    "- Top-k(3): ranked candidate names with confidence, enables analyst-shortlist analysis\n",
    "- All v2 prompt templates, no function limit\n",
    "\n",
    "### Model-Aware Routing\n",
    "> The LLM runner uses `workers.llm.model_router` to automatically adapt API\n",
    "> calls per provider: response_format handling, Anthropic beta headers,\n",
    "> DeepSeek reasoning token stripping, and `require_parameters` provider routing.\n",
    "\n",
    "### Non-Negotiable Constraint\n",
    "> **LLM input MUST contain ONLY Ghidra-derived artefacts.**  \n",
    "> Ground truth (DWARF / source / join metadata) is used ONLY post-hoc for scoring.  \n",
    "> The `/llm/functions` endpoint enforces this by construction.\n",
    "\n",
    "### Prerequisites\n",
    "- Docker stack running: `docker compose up -d` in `reforge/docker/`\n",
    "- Services: `api` (port 8080), `redis`, `postgres`\n",
    "- `OPENROUTER_API_KEY` set in `docker/.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2726fb5",
   "metadata": {},
   "source": [
    "## §1 — Setup & Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065024da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API: {'status': 'healthy', 'service': 'reforge-api', 'version': '0.1.0'}\n",
      "Key: ...e0ea6829\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json, time, importlib, textwrap\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import requests\n",
    "\n",
    "# Ensure reforge root is on sys.path\n",
    "REFORGE_ROOT = Path(\".\").resolve().parent\n",
    "if str(REFORGE_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REFORGE_ROOT))\n",
    "\n",
    "API = \"http://localhost:8080\"\n",
    "OPENROUTER_KEY = os.environ.get(\n",
    "    \"OPENROUTER_API_KEY\",\n",
    "    \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    ")\n",
    "\n",
    "def api(path, **kw):  return requests.get(f\"{API}{path}\", **kw).json()\n",
    "def post(url, body):  return requests.post(url, json=body).json()\n",
    "\n",
    "health = api(\"/health\")\n",
    "print(f\"API: {health}\")\n",
    "print(f\"Key: ...{OPENROUTER_KEY[-8:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb544e3",
   "metadata": {},
   "source": [
    "## §2 — Review Legacy Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b7c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 53\n",
      "Legacy (pilot):    5\n",
      "Active:            48\n",
      "\n",
      "  OLD  exp01_funcnaming_gpt4omini_gold_O0                  openai/gpt-4o-mini\n",
      "  OLD  exp02_funcnaming_gpt4o_gold_O0                      openai/gpt-4o\n",
      "  OLD  exp03_funcnaming_claude_gold_O0                     anthropic/claude-3.5-sonnet\n",
      "  OLD  exp04_funcnaming_gpt4omini_gold_O2                  openai/gpt-4o-mini\n",
      "  OLD  exp05_funcnaming_gpt4omini_silver_O0                openai/gpt-4o-mini\n",
      "\n",
      "  RDY  bench_gpt4o-mini_gold_O0_L2_topk3                   openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_gold_O1_L2_topk3                   openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_gold_O2_L2_topk3                   openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_gold_O3_L2_topk3                   openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_silver_O0_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_silver_O1_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_silver_O2_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_silver_O3_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_bronze_O0_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_bronze_O1_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_bronze_O2_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_gpt4o-mini_bronze_O3_L2_topk3                 openai/gpt-4o-mini                   L2\n",
      "  RDY  bench_deepseek-v3_gold_O0_L2_topk3                  deepseek/deepseek-chat-v3-0324       L2\n",
      "  RDY  bench_deepseek-v3_gold_O1_L2_topk3                  deepseek/deepseek-chat-v3-0324       L2\n",
      "  RDY  bench_deepseek-v3_gold_O2_L2_topk3                  deepseek/deepseek-chat-v3-0324       L2\n",
      "  RDY  bench_deepseek-v3_gold_O3_L2_topk3                  deepseek/deepseek-chat-v3-0324       L2\n",
      "  RDY  bench_deepseek-v3_silver_O0_L2_topk3                deepseek/deepseek-chat-v3-0324       L2\n",
      "  RDY  bench_deepseek-v3_silver_O1_L2_topk3                deepseek/deepseek-chat-v3-0324       L2\n",
      "  RDY  bench_deepseek-v3_silver_O2_L2_topk3                deepseek/deepseek-chat-v3-0324       L2\n",
      "  RDY  bench_deepseek-v3_silver_O3_L2_topk3                deepseek/deepseek-chat-v3-0324       L2\n"
     ]
    }
   ],
   "source": [
    "experiments = api(\"/data/experiments\")\n",
    "\n",
    "legacy = [e for e in experiments if e.get('status') == 'legacy']\n",
    "active = [e for e in experiments if e.get('status') != 'legacy']\n",
    "\n",
    "print(f\"Total experiments: {len(experiments)}\")\n",
    "print(f\"Legacy (pilot):    {len(legacy)}\")\n",
    "print(f\"Active:            {len(active)}\")\n",
    "print()\n",
    "for e in legacy:\n",
    "    print(f\"  OLD  {e['id']:50s}  {e['model']}\")\n",
    "print()\n",
    "for e in active[:20]:\n",
    "    status_icon = {'ready': 'RDY', 'draft': 'DFT', 'running': 'RUN', 'completed': 'DON'}.get(e['status'], '???')\n",
    "    print(f\"  {status_icon}  {e['id']:50s}  {e['model']:35s}  {e.get('context_level', 'L0')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea7376",
   "metadata": {},
   "source": [
    "## §3 — Build the Thesis Benchmark Matrix\n",
    "\n",
    "Generate all 36 experiment configs:\n",
    "- 3 models x 3 tiers x 4 opts x L2 x top-k(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169b0a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thesis benchmark matrix: 84 experiments\n",
      "  Models:  ['gpt4o-mini', 'deepseek-v3', 'claude-sonnet45', 'llama31-70b', 'deepseek-r1', 'qwen3-coder', 'gpt51']\n",
      "  Tiers:   GOLD, SILVER, BRONZE\n",
      "  Opts:    O0, O1, O2, O3\n",
      "  Context: L2 (max Ghidra structural data)\n",
      "  Top-k:   3 (JSON structured output)\n",
      "\n",
      "  bench_gpt4o-mini_gold_O0_L2_topk3                       -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_gold_O1_L2_topk3                       -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_gold_O2_L2_topk3                       -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_gold_O3_L2_topk3                       -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_silver_O0_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_silver_O1_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_silver_O2_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_silver_O3_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_bronze_O0_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_bronze_O1_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_bronze_O2_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_gpt4o-mini_bronze_O3_L2_topk3                     -> openai/gpt-4o-mini                  top_k=3\n",
      "  bench_deepseek-v3_gold_O0_L2_topk3                      -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_gold_O1_L2_topk3                      -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_gold_O2_L2_topk3                      -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_gold_O3_L2_topk3                      -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_silver_O0_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_silver_O1_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_silver_O2_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_silver_O3_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_bronze_O0_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_bronze_O1_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_bronze_O2_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_deepseek-v3_bronze_O3_L2_topk3                    -> deepseek/deepseek-chat-v3-0324      top_k=3\n",
      "  bench_claude-sonnet45_gold_O0_L2_topk3                  -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_gold_O1_L2_topk3                  -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_gold_O2_L2_topk3                  -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_gold_O3_L2_topk3                  -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_silver_O0_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_silver_O1_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_silver_O2_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_silver_O3_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_bronze_O0_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_bronze_O1_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_bronze_O2_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_claude-sonnet45_bronze_O3_L2_topk3                -> anthropic/claude-sonnet-4.5         top_k=3\n",
      "  bench_llama31-70b_gold_O0_L2_topk3                      -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_gold_O1_L2_topk3                      -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_gold_O2_L2_topk3                      -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_gold_O3_L2_topk3                      -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_silver_O0_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_silver_O1_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_silver_O2_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_silver_O3_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_bronze_O0_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_bronze_O1_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_bronze_O2_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_llama31-70b_bronze_O3_L2_topk3                    -> meta-llama/llama-3.1-70b-instruct   top_k=3\n",
      "  bench_deepseek-r1_gold_O0_L2_topk3                      -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_gold_O1_L2_topk3                      -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_gold_O2_L2_topk3                      -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_gold_O3_L2_topk3                      -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_silver_O0_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_silver_O1_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_silver_O2_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_silver_O3_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_bronze_O0_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_bronze_O1_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_bronze_O2_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_deepseek-r1_bronze_O3_L2_topk3                    -> deepseek/deepseek-r1-0528           top_k=3\n",
      "  bench_qwen3-coder_gold_O0_L2_topk3                      -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_gold_O1_L2_topk3                      -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_gold_O2_L2_topk3                      -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_gold_O3_L2_topk3                      -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_silver_O0_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_silver_O1_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_silver_O2_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_silver_O3_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_bronze_O0_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_bronze_O1_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_bronze_O2_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_qwen3-coder_bronze_O3_L2_topk3                    -> qwen/qwen3-coder                    top_k=3\n",
      "  bench_gpt51_gold_O0_L2_topk3                            -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_gold_O1_L2_topk3                            -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_gold_O2_L2_topk3                            -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_gold_O3_L2_topk3                            -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_silver_O0_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_silver_O1_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_silver_O2_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_silver_O3_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_bronze_O0_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_bronze_O1_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_bronze_O2_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "  bench_gpt51_bronze_O3_L2_topk3                          -> openai/gpt-5.1                      top_k=3\n",
      "\n",
      "Registering experiments with API server...\n",
      "  Registered 84 experiments (0 new, 84 updated)\n"
     ]
    }
   ],
   "source": [
    "from data.experiments import (\n",
    "    build_thesis_matrix,\n",
    "    build_benchmark_matrix,\n",
    "    estimate_benchmark_cost,\n",
    "    THESIS_MODELS,\n",
    "    BENCHMARK_TIERS,\n",
    "    BENCHMARK_OPTS,\n",
    "    REGISTRY,\n",
    ")\n",
    "\n",
    "# Build the thesis matrix (36 experiments)\n",
    "matrix = build_thesis_matrix(register=True)\n",
    "\n",
    "print(f\"Thesis benchmark matrix: {len(matrix)} experiments\")\n",
    "print(f\"  Models:  {list(THESIS_MODELS.keys())}\")\n",
    "print(f\"  Tiers:   GOLD, SILVER, BRONZE\")\n",
    "print(f\"  Opts:    O0, O1, O2, O3\")\n",
    "print(f\"  Context: L2 (max Ghidra structural data)\")\n",
    "print(f\"  Top-k:   3 (JSON structured output)\")\n",
    "print()\n",
    "for cfg in matrix:\n",
    "    print(f\"  {cfg.id:55s} -> {cfg.model:35s} top_k={cfg.top_k}\")\n",
    "\n",
    "# Push matrix to the API server\n",
    "print()\n",
    "print(\"Registering experiments with API server...\")\n",
    "resp = requests.post(\n",
    "    f\"{API}/data/experiments/bulk\",\n",
    "    json=[cfg.model_dump() for cfg in matrix],\n",
    ")\n",
    "if resp.status_code in (200, 201):\n",
    "    reg = resp.json()\n",
    "    print(f\"  Registered {reg['registered']} experiments ({reg['created']} new, {reg['updated']} updated)\")\n",
    "else:\n",
    "    print(f\"  Registration failed: {resp.status_code} - {resp.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58803c04",
   "metadata": {},
   "source": [
    "## §4 — Budget Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15eda0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions per (tier, opt):\n",
      "  BRONZE   O0 :    0 functions\n",
      "  BRONZE   O1 :   17 functions\n",
      "  BRONZE   O2 :   30 functions\n",
      "  BRONZE   O3 :   33 functions\n",
      "  GOLD     O0 :  197 functions\n",
      "  GOLD     O1 :  121 functions\n",
      "  GOLD     O2 :   92 functions\n",
      "  GOLD     O3 :   85 functions\n",
      "  SILVER   O0 :   29 functions\n",
      "  SILVER   O1 :   23 functions\n",
      "  SILVER   O2 :   32 functions\n",
      "  SILVER   O3 :   34 functions\n",
      "\n",
      "Average per experiment: ~57 functions\n",
      "\n",
      "Total experiments:   48\n",
      "Total LLM calls:    2,736\n",
      "Total input tokens:  2,188,800\n",
      "Estimated cost:      $1.61\n",
      "\n",
      "Within $300 budget - $298.39 remaining\n",
      "\n",
      "Per-model cost:\n",
      "  $    1.37  anthropic/claude-sonnet-4.5\n",
      "  $    0.08  meta-llama/llama-3.1-70b-instruct\n",
      "  $    0.08  deepseek/deepseek-r1-0528\n",
      "  $    0.08  qwen/qwen3-coder\n"
     ]
    }
   ],
   "source": [
    "# Count functions per tier/opt\n",
    "func_counts = {}\n",
    "for tier in [\"GOLD\", \"SILVER\", \"BRONZE\"]:\n",
    "    for opt_lvl in [\"O0\", \"O1\", \"O2\", \"O3\"]:\n",
    "        try:\n",
    "            fns = api(\"/llm/functions\", params={\"opt\": opt_lvl, \"tier\": tier, \"limit\": 5000})\n",
    "            func_counts[(tier, opt_lvl)] = len(fns)\n",
    "        except Exception:\n",
    "            func_counts[(tier, opt_lvl)] = 0\n",
    "\n",
    "total_functions_per_exp = sum(func_counts.values()) // max(len(func_counts), 1)\n",
    "print(\"Functions per (tier, opt):\")\n",
    "for (t, o), n in sorted(func_counts.items()):\n",
    "    print(f\"  {t:8s} {o:3s}: {n:4d} functions\")\n",
    "print(f\"\\nAverage per experiment: ~{total_functions_per_exp} functions\")\n",
    "print()\n",
    "\n",
    "# top-k uses ~80 completion tokens (JSON overhead) vs ~20 for single-name\n",
    "est = estimate_benchmark_cost(\n",
    "    matrix,\n",
    "    avg_prompt_tokens=800,\n",
    "    avg_completion_tokens=80,\n",
    "    functions_per_experiment=max(total_functions_per_exp, 50),\n",
    ")\n",
    "\n",
    "print(f\"Total experiments:   {est['total_experiments']}\")\n",
    "print(f\"Total LLM calls:    {est['total_calls']:,}\")\n",
    "print(f\"Total input tokens:  {est['total_input_tokens']:,}\")\n",
    "print(f\"Estimated cost:      ${est['estimated_cost_usd']:.2f}\")\n",
    "print()\n",
    "\n",
    "BUDGET = 300.0\n",
    "if est['estimated_cost_usd'] > BUDGET:\n",
    "    print(f\"OVER BUDGET (${BUDGET:.0f}). Narrow the matrix or reduce models.\")\n",
    "else:\n",
    "    print(f\"Within ${BUDGET:.0f} budget - ${BUDGET - est['estimated_cost_usd']:.2f} remaining\")\n",
    "\n",
    "# Per-model breakdown\n",
    "model_costs = {}\n",
    "for item in est['breakdown']:\n",
    "    m = item['model']\n",
    "    model_costs[m] = model_costs.get(m, 0) + item['est_cost_usd']\n",
    "print(\"\\nPer-model cost:\")\n",
    "for m, c in sorted(model_costs.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  ${c:8.2f}  {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0842d7",
   "metadata": {},
   "source": [
    "## §5 — Dry-Run Validation\n",
    "\n",
    "Validate prompt rendering, JSON parsing, and API round-trip **without** making real LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59a9e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: cu0x0:die0x12d\n",
      "  ghidra_name:  FUN_004011fa\n",
      "  test_case:    t01_crossfile_calls\n",
      "\n",
      "================================================================================\n",
      "FULL RENDERED PROMPT (top-k L2)\n",
      "================================================================================\n",
      "You are an expert reverse engineer analyzing decompiled binary code.\n",
      "\n",
      "A function has been decompiled from a stripped binary using Ghidra. The original\n",
      "symbol names have been removed by the strip tool. Your task is to analyze the\n",
      "decompiled C code along with its structural metadata, then suggest multiple\n",
      "candidate function names ranked by your confidence.\n",
      "\n",
      "Guidelines:\n",
      "- Use snake_case naming convention\n",
      "- Be specific but concise — prefer 2-4 words\n",
      "- Focus on the function's PURPOSE, not its implementation details\n",
      "- Use call relationships and variable information to understand context\n",
      "- Consider control-flow complexity when reasoning about function role\n",
      "- If the function is a standard library wrapper, name it accordingly\n",
      "- If you cannot determine the purpose, use a descriptive structural name\n",
      "\n",
      "Return EXACTLY 3 candidate names ranked from most to least confident.\n",
      "Respond with ONLY a JSON object in this exact format, nothing else:\n",
      "\n",
      "{\"predictions\": [{\"name\": \"best_name\", \"confidence\": 0.9}, {\"name\": \"second_name\", \"confidence\": 0.6}, {\"name\": \"third_name\", \"confidence\": 0.3}]}\n",
      "\n",
      "Rules:\n",
      "- Confidence values must be between 0.0 and 1.0, descending\n",
      "- All names must be valid snake_case identifiers\n",
      "- No explanation, no markdown, no text outside the JSON object\n",
      "\n",
      "## Decompiled Code\n",
      "\n",
      "```c\n",
      "\n",
      "int FUN_004011fa(int *param_1,int param_2)\n",
      "\n",
      "{\n",
      "  int local_10;\n",
      "  int local_c;\n",
      "  \n",
      "  if (param_2 < 1) {\n",
      "    local_c = 0;\n",
      "  }\n",
      "  else {\n",
      "    local_c = *param_1;\n",
      "    for (local_10 = 1; local_10 < param_2; local_10 = local_10 + 1) {\n",
      "      if (param_1[local_10] < local_c) {\n",
      "        local_c = param_1[local_10];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  return local_c;\n",
      "}\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## Call Relationships\n",
      "\n",
      "(no outgoing calls)\n",
      "\n",
      "## Control Flow Summary\n",
      "\n",
      "Control-flow summary:\n",
      "  - Basic blocks: 9\n",
      "  - Edges: 11\n",
      "  - Cyclomatic complexity: 4\n",
      "  - Has indirect jumps: no\n",
      "  - CFG completeness: HIGH\n",
      "\n",
      "## Local Variables\n",
      "\n",
      "Variables:\n",
      "  - local_10: int (LOCAL, STACK)\n",
      "  - local_c: int (LOCAL, STACK)\n",
      "  - param_2: int (PARAM, REGISTER)\n",
      "  - param_1: int * (PARAM, REGISTER)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 2019 chars  (~504 tokens)\n",
      "\n",
      "-- Parser test --\n",
      "  parse_ok: True\n",
      "  predictions: [{'name': 'parse_header', 'confidence': 0.9}, {'name': 'read_input', 'confidence': 0.6}, {'name': 'process_data', 'confidence': 0.3}]\n",
      "  top-1: parse_header\n"
     ]
    }
   ],
   "source": [
    "## Preview: render one full top-k prompt at L2\n",
    "\n",
    "from workers.llm.prompt import load_template, render_prompt\n",
    "from workers.llm.response_parser import parse_topk_response\n",
    "\n",
    "# Fetch one L2 function from the API\n",
    "sample = api(\"/llm/functions\", params={\n",
    "    \"opt\": \"O0\",\n",
    "    \"tier\": \"GOLD\",\n",
    "    \"context_level\": \"L2\",\n",
    "    \"limit\": 1,\n",
    "})\n",
    "\n",
    "if not sample:\n",
    "    print(\"No functions returned - is the data loaded?\")\n",
    "else:\n",
    "    fn = sample[0]\n",
    "    print(f\"Function: {fn['dwarf_function_id']}\")\n",
    "    print(f\"  ghidra_name:  {fn.get('ghidra_name')}\")\n",
    "    print(f\"  test_case:    {fn.get('test_case')}\")\n",
    "    print()\n",
    "\n",
    "    # Load the top-k L2 template and render\n",
    "    template = load_template(\"function_naming_topk_L2\")\n",
    "    prompt = render_prompt(\n",
    "        template,\n",
    "        fn.get(\"c_raw\", \"\"),\n",
    "        calls=fn.get(\"calls_text\"),\n",
    "        cfg_summary=fn.get(\"cfg_text\"),\n",
    "        variables=fn.get(\"variables_text\"),\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FULL RENDERED PROMPT (top-k L2)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nPrompt length: {len(prompt)} chars  (~{len(prompt)//4} tokens)\")\n",
    "\n",
    "    # Test the parser on a simulated response\n",
    "    print(\"\\n-- Parser test --\")\n",
    "    fake_response = '{\"predictions\": [{\"name\": \"parse_header\", \"confidence\": 0.9}, {\"name\": \"read_input\", \"confidence\": 0.6}, {\"name\": \"process_data\", \"confidence\": 0.3}]}'\n",
    "    parsed = parse_topk_response(fake_response)\n",
    "    print(f\"  parse_ok: {parsed.parse_ok}\")\n",
    "    print(f\"  predictions: {parsed.predictions}\")\n",
    "    print(f\"  top-1: {parsed.predictions[0]['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776dc652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRY RUN: bench_claude-sonnet45_gold_O0_L2_topk3\n",
      "============================================================\n",
      "  Total: 197, New: 197, Errors: 0\n",
      "  PASS\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from workers.llm.runner import run_experiment\n",
    "\n",
    "# Pick one experiment for dry-run validation\n",
    "dry_run_id = matrix[0].id\n",
    "\n",
    "print(f\"DRY RUN: {dry_run_id}\")\n",
    "print(\"=\" * 60)\n",
    "summary = await run_experiment(\n",
    "    dry_run_id,\n",
    "    api_base=API,\n",
    "    dry_run=True,\n",
    ")\n",
    "print(f\"  Total: {summary['total']}, New: {summary['new']}, Errors: {summary['errors']}\")\n",
    "print(f\"  {'PASS' if summary['errors'] == 0 else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fc73a",
   "metadata": {},
   "source": [
    "## §6 — Execute Experiments\n",
    "\n",
    "### Execution Strategy\n",
    "- **Phase 1**: gpt-4o-mini (12 experiments) — cheapest, validates full pipeline\n",
    "- **Phase 2**: deepseek-v3-0324 (12 experiments) — code specialist (replaces discontinued coder-v2)\n",
    "- **Phase 3**: gpt-5.1 (12 experiments) — premium thinking model\n",
    "\n",
    "Each phase can be run independently. Results are idempotent (resume support).\n",
    "The runner now pre-checks model availability and uses model-aware routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c9a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase experiments: 48\n",
      "  bench_claude-sonnet45_gold_O0_L2_topk3\n",
      "  bench_claude-sonnet45_gold_O1_L2_topk3\n",
      "  bench_claude-sonnet45_gold_O2_L2_topk3\n",
      "  bench_claude-sonnet45_gold_O3_L2_topk3\n",
      "  bench_claude-sonnet45_silver_O0_L2_topk3\n",
      "  bench_claude-sonnet45_silver_O1_L2_topk3\n",
      "  bench_claude-sonnet45_silver_O2_L2_topk3\n",
      "  bench_claude-sonnet45_silver_O3_L2_topk3\n",
      "  bench_claude-sonnet45_bronze_O0_L2_topk3\n",
      "  bench_claude-sonnet45_bronze_O1_L2_topk3\n",
      "  bench_claude-sonnet45_bronze_O2_L2_topk3\n",
      "  bench_claude-sonnet45_bronze_O3_L2_topk3\n",
      "  bench_llama31-70b_gold_O0_L2_topk3\n",
      "  bench_llama31-70b_gold_O1_L2_topk3\n",
      "  bench_llama31-70b_gold_O2_L2_topk3\n",
      "  bench_llama31-70b_gold_O3_L2_topk3\n",
      "  bench_llama31-70b_silver_O0_L2_topk3\n",
      "  bench_llama31-70b_silver_O1_L2_topk3\n",
      "  bench_llama31-70b_silver_O2_L2_topk3\n",
      "  bench_llama31-70b_silver_O3_L2_topk3\n",
      "  bench_llama31-70b_bronze_O0_L2_topk3\n",
      "  bench_llama31-70b_bronze_O1_L2_topk3\n",
      "  bench_llama31-70b_bronze_O2_L2_topk3\n",
      "  bench_llama31-70b_bronze_O3_L2_topk3\n",
      "  bench_deepseek-r1_gold_O0_L2_topk3\n",
      "  bench_deepseek-r1_gold_O1_L2_topk3\n",
      "  bench_deepseek-r1_gold_O2_L2_topk3\n",
      "  bench_deepseek-r1_gold_O3_L2_topk3\n",
      "  bench_deepseek-r1_silver_O0_L2_topk3\n",
      "  bench_deepseek-r1_silver_O1_L2_topk3\n",
      "  bench_deepseek-r1_silver_O2_L2_topk3\n",
      "  bench_deepseek-r1_silver_O3_L2_topk3\n",
      "  bench_deepseek-r1_bronze_O0_L2_topk3\n",
      "  bench_deepseek-r1_bronze_O1_L2_topk3\n",
      "  bench_deepseek-r1_bronze_O2_L2_topk3\n",
      "  bench_deepseek-r1_bronze_O3_L2_topk3\n",
      "  bench_qwen3-coder_gold_O0_L2_topk3\n",
      "  bench_qwen3-coder_gold_O1_L2_topk3\n",
      "  bench_qwen3-coder_gold_O2_L2_topk3\n",
      "  bench_qwen3-coder_gold_O3_L2_topk3\n",
      "  bench_qwen3-coder_silver_O0_L2_topk3\n",
      "  bench_qwen3-coder_silver_O1_L2_topk3\n",
      "  bench_qwen3-coder_silver_O2_L2_topk3\n",
      "  bench_qwen3-coder_silver_O3_L2_topk3\n",
      "  bench_qwen3-coder_bronze_O0_L2_topk3\n",
      "  bench_qwen3-coder_bronze_O1_L2_topk3\n",
      "  bench_qwen3-coder_bronze_O2_L2_topk3\n",
      "  bench_qwen3-coder_bronze_O3_L2_topk3\n"
     ]
    }
   ],
   "source": [
    "# Phase selector - uncomment ONE phase at a time\n",
    "\n",
    "    # \"claude-sonnet45\":  \"anthropic/claude-sonnet-4.5\", \n",
    "    # \"llama31-70b\":      \"meta-llama/llama-3.1-70b-instruct\",\n",
    "    # \"deepseek-r1\":      \"deepseek/deepseek-r1-0528\",\n",
    "    # \"qwen3-coder\":      \"qwen/qwen3-coder\",\n",
    "\n",
    "\n",
    "# Phase 1: cheap (validate pipeline)\n",
    "#phase_filter = {\"gpt4o-mini\"}\n",
    "# Phase 2: code specialist (was deepseek-coder2, now deepseek-v3)\n",
    "# phase_filter = {\"deepseek-v3\"}\n",
    "# Phase 3: premium\n",
    "# phase_filter = {\"gpt51\"}\n",
    "# All at once:\n",
    "phase_filter = {\"qwen3-coder\", \"deepseek-r1\", \"llama31-70b\", \"claude-sonnet45\"}\n",
    "\n",
    "\n",
    "phase_experiments = [\n",
    "    cfg for cfg in matrix\n",
    "    if any(label in cfg.id for label in phase_filter)\n",
    "]\n",
    "print(f\"Phase experiments: {len(phase_experiments)}\")\n",
    "for cfg in phase_experiments:\n",
    "    print(f\"  {cfg.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a67a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/48] bench_claude-sonnet45_gold_O0_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: GOLD  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 197/197 [01:41<00:00,  1.95fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 196, New: 196, Errors: 0\n",
      "\n",
      "[2/48] bench_claude-sonnet45_gold_O1_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: GOLD  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 121/121 [01:02<00:00,  1.93fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 121, New: 121, Errors: 0\n",
      "\n",
      "[3/48] bench_claude-sonnet45_gold_O2_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: GOLD  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 92/92 [00:47<00:00,  1.92fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 92, New: 92, Errors: 0\n",
      "\n",
      "[4/48] bench_claude-sonnet45_gold_O3_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: GOLD  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 85/85 [00:44<00:00,  1.91fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 85, New: 85, Errors: 0\n",
      "\n",
      "[5/48] bench_claude-sonnet45_silver_O0_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: SILVER  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 29/29 [00:15<00:00,  1.83fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 29, New: 29, Errors: 0\n",
      "\n",
      "[6/48] bench_claude-sonnet45_silver_O1_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: SILVER  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 23/23 [00:12<00:00,  1.79fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 23, New: 23, Errors: 0\n",
      "\n",
      "[7/48] bench_claude-sonnet45_silver_O2_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: SILVER  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 32/32 [00:17<00:00,  1.78fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 32, New: 32, Errors: 0\n",
      "\n",
      "[8/48] bench_claude-sonnet45_silver_O3_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: SILVER  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 34/34 [00:20<00:00,  1.66fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 34, New: 34, Errors: 0\n",
      "\n",
      "[9/48] bench_claude-sonnet45_bronze_O0_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: BRONZE  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No functions to process — exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 0, New: 0, Errors: 0\n",
      "\n",
      "[10/48] bench_claude-sonnet45_bronze_O1_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: BRONZE  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 17/17 [00:09<00:00,  1.73fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 17, New: 17, Errors: 0\n",
      "\n",
      "[11/48] bench_claude-sonnet45_bronze_O2_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: BRONZE  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 30/30 [00:15<00:00,  1.89fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 30, New: 30, Errors: 0\n",
      "\n",
      "[12/48] bench_claude-sonnet45_bronze_O3_L2_topk3\n",
      "  Model: anthropic/claude-sonnet-4.5  Tier: BRONZE  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (anthropic/claude-sonnet-4.5): 100%|██████████| 33/33 [00:18<00:00,  1.78fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 33, New: 33, Errors: 0\n",
      "\n",
      "[13/48] bench_llama31-70b_gold_O0_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: GOLD  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 197/197 [01:33<00:00,  2.11fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 196, New: 196, Errors: 0\n",
      "\n",
      "[14/48] bench_llama31-70b_gold_O1_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: GOLD  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 121/121 [00:55<00:00,  2.16fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 121, New: 121, Errors: 0\n",
      "\n",
      "[15/48] bench_llama31-70b_gold_O2_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: GOLD  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 92/92 [00:47<00:00,  1.94fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 92, New: 92, Errors: 0\n",
      "\n",
      "[16/48] bench_llama31-70b_gold_O3_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: GOLD  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 85/85 [00:39<00:00,  2.15fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 85, New: 85, Errors: 0\n",
      "\n",
      "[17/48] bench_llama31-70b_silver_O0_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: SILVER  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 29/29 [00:22<00:00,  1.31fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 29, New: 29, Errors: 0\n",
      "\n",
      "[18/48] bench_llama31-70b_silver_O1_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: SILVER  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 23/23 [00:11<00:00,  2.00fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 23, New: 23, Errors: 0\n",
      "\n",
      "[19/48] bench_llama31-70b_silver_O2_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: SILVER  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 32/32 [00:13<00:00,  2.30fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 32, New: 32, Errors: 0\n",
      "\n",
      "[20/48] bench_llama31-70b_silver_O3_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: SILVER  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 34/34 [00:28<00:00,  1.20fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 34, New: 34, Errors: 0\n",
      "\n",
      "[21/48] bench_llama31-70b_bronze_O0_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: BRONZE  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No functions to process — exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 0, New: 0, Errors: 0\n",
      "\n",
      "[22/48] bench_llama31-70b_bronze_O1_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: BRONZE  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 17/17 [00:08<00:00,  1.92fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 17, New: 17, Errors: 0\n",
      "\n",
      "[23/48] bench_llama31-70b_bronze_O2_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: BRONZE  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 30/30 [00:17<00:00,  1.73fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 30, New: 30, Errors: 0\n",
      "\n",
      "[24/48] bench_llama31-70b_bronze_O3_L2_topk3\n",
      "  Model: meta-llama/llama-3.1-70b-instruct  Tier: BRONZE  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (meta-llama/llama-3.1-70b-instruct): 100%|██████████| 33/33 [00:19<00:00,  1.73fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 33, New: 33, Errors: 0\n",
      "\n",
      "[25/48] bench_deepseek-r1_gold_O0_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: GOLD  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 197/197 [37:00<00:00, 11.27s/fn] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 196, New: 196, Errors: 0\n",
      "\n",
      "[26/48] bench_deepseek-r1_gold_O1_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: GOLD  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528):  86%|████████▌ | 104/121 [37:16<06:33, 23.17s/fn]LLM call failed for cu0x1167:die0x1442: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "LLM calls (deepseek/deepseek-r1-0528):  90%|█████████ | 109/121 [39:39<04:44, 23.68s/fn]LLM call failed for cu0x87f:die0xd40: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 121/121 [46:28<00:00, 23.05s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARN Completed: 119, New: 119, Errors: 2\n",
      "\n",
      "[27/48] bench_deepseek-r1_gold_O2_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: GOLD  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528):  37%|███▋      | 34/92 [15:56<29:03, 30.07s/fn] LLM call failed for cu0x1391:die0x166c: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "LLM calls (deepseek/deepseek-r1-0528):  38%|███▊      | 35/92 [16:24<27:56, 29.41s/fn]LLM call failed for cu0x0:die0x547: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "LLM calls (deepseek/deepseek-r1-0528):  92%|█████████▏| 85/92 [35:06<02:03, 17.66s/fn]LLM call failed for cu0xda4:die0x10ef: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "LLM calls (deepseek/deepseek-r1-0528):  93%|█████████▎| 86/92 [35:49<02:32, 25.35s/fn]LLM call failed for cu0x0:die0x41c: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 92/92 [40:58<00:00, 26.72s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARN Completed: 88, New: 88, Errors: 4\n",
      "\n",
      "[28/48] bench_deepseek-r1_gold_O3_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: GOLD  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528):  80%|████████  | 68/85 [20:22<04:32, 16.06s/fn]LLM call failed for cu0x7cc:die0x9fd: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 85/85 [35:18<00:00, 24.93s/fn] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARN Completed: 84, New: 84, Errors: 1\n",
      "\n",
      "[29/48] bench_deepseek-r1_silver_O0_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: SILVER  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 29/29 [11:19<00:00, 23.43s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 29, New: 29, Errors: 0\n",
      "\n",
      "[30/48] bench_deepseek-r1_silver_O1_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: SILVER  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 23/23 [08:27<00:00, 22.08s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 23, New: 23, Errors: 0\n",
      "\n",
      "[31/48] bench_deepseek-r1_silver_O2_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: SILVER  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 32/32 [17:23<00:00, 32.60s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 32, New: 32, Errors: 0\n",
      "\n",
      "[32/48] bench_deepseek-r1_silver_O3_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: SILVER  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 34/34 [13:26<00:00, 23.73s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 34, New: 34, Errors: 0\n",
      "\n",
      "[33/48] bench_deepseek-r1_bronze_O0_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: BRONZE  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No functions to process — exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 0, New: 0, Errors: 0\n",
      "\n",
      "[34/48] bench_deepseek-r1_bronze_O1_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: BRONZE  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 17/17 [04:09<00:00, 14.69s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 17, New: 17, Errors: 0\n",
      "\n",
      "[35/48] bench_deepseek-r1_bronze_O2_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: BRONZE  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 30/30 [11:51<00:00, 23.73s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 30, New: 30, Errors: 0\n",
      "\n",
      "[36/48] bench_deepseek-r1_bronze_O3_L2_topk3\n",
      "  Model: deepseek/deepseek-r1-0528  Tier: BRONZE  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (deepseek/deepseek-r1-0528): 100%|██████████| 33/33 [12:01<00:00, 21.87s/fn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 33, New: 33, Errors: 0\n",
      "\n",
      "[37/48] bench_qwen3-coder_gold_O0_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: GOLD  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 197/197 [00:32<00:00,  6.04fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 196, New: 196, Errors: 0\n",
      "\n",
      "[38/48] bench_qwen3-coder_gold_O1_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: GOLD  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 121/121 [00:17<00:00,  6.83fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 121, New: 121, Errors: 0\n",
      "\n",
      "[39/48] bench_qwen3-coder_gold_O2_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: GOLD  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 92/92 [00:13<00:00,  6.81fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 92, New: 92, Errors: 0\n",
      "\n",
      "[40/48] bench_qwen3-coder_gold_O3_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: GOLD  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 85/85 [00:12<00:00,  6.84fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 85, New: 85, Errors: 0\n",
      "\n",
      "[41/48] bench_qwen3-coder_silver_O0_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: SILVER  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 29/29 [00:04<00:00,  6.72fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 29, New: 29, Errors: 0\n",
      "\n",
      "[42/48] bench_qwen3-coder_silver_O1_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: SILVER  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 23/23 [00:03<00:00,  6.35fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 23, New: 23, Errors: 0\n",
      "\n",
      "[43/48] bench_qwen3-coder_silver_O2_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: SILVER  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 32/32 [00:05<00:00,  6.28fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 32, New: 32, Errors: 0\n",
      "\n",
      "[44/48] bench_qwen3-coder_silver_O3_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: SILVER  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 34/34 [00:05<00:00,  6.66fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 34, New: 34, Errors: 0\n",
      "\n",
      "[45/48] bench_qwen3-coder_bronze_O0_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: BRONZE  Opt: O0  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No functions to process — exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 0, New: 0, Errors: 0\n",
      "\n",
      "[46/48] bench_qwen3-coder_bronze_O1_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: BRONZE  Opt: O1  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 17/17 [00:02<00:00,  5.87fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 17, New: 17, Errors: 0\n",
      "\n",
      "[47/48] bench_qwen3-coder_bronze_O2_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: BRONZE  Opt: O2  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 30/30 [00:04<00:00,  7.08fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 30, New: 30, Errors: 0\n",
      "\n",
      "[48/48] bench_qwen3-coder_bronze_O3_L2_topk3\n",
      "  Model: qwen/qwen3-coder  Tier: BRONZE  Opt: O3  Top-k: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM calls (qwen/qwen3-coder): 100%|██████████| 33/33 [00:05<00:00,  6.59fn/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK Completed: 33, New: 33, Errors: 0\n",
      "\n",
      "============================================================\n",
      "Phase complete: 48 experiments, 7 total errors\n"
     ]
    }
   ],
   "source": [
    "# Execute the selected phase\n",
    "# Each experiment uses internal async concurrency (5 parallel LLM calls).\n",
    "\n",
    "from workers.llm.runner import run_experiment\n",
    "\n",
    "summaries = []\n",
    "errors_total = 0\n",
    "\n",
    "for i, cfg in enumerate(phase_experiments, 1):\n",
    "    print(f\"\\n[{i}/{len(phase_experiments)}] {cfg.id}\")\n",
    "    print(f\"  Model: {cfg.model}  Tier: {cfg.tier}  Opt: {cfg.opt}  Top-k: {cfg.top_k}\")\n",
    "    \n",
    "    try:\n",
    "        summary = await run_experiment(\n",
    "            cfg.id,\n",
    "            api_base=API,\n",
    "            openrouter_key=OPENROUTER_KEY,\n",
    "            concurrency=5,\n",
    "        )\n",
    "        summaries.append(summary)\n",
    "        \n",
    "        new = summary.get('new', 0)\n",
    "        errs = summary.get('errors', 0)\n",
    "        errors_total += errs\n",
    "        status = 'OK' if errs == 0 else 'WARN'\n",
    "        print(f\"  {status} Completed: {summary.get('completed', 0)}, New: {new}, Errors: {errs}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  FAILED: {exc}\")\n",
    "        errors_total += 1\n",
    "        summaries.append({\"experiment_id\": cfg.id, \"error\": str(exc)})\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Phase complete: {len(summaries)} experiments, {errors_total} total errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784d410",
   "metadata": {},
   "source": [
    "## §7 — Score All Results\n",
    "\n",
    "Trigger the scorer for all benchmark experiments. The scorer:\n",
    "1. Joins ground truth post-hoc (leak-proof)\n",
    "2. Enriches with stable keys for cross-opt pairing\n",
    "3. Computes top-1 AND top-k metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea16abb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK bench_gpt4o-mini_gold_O0_L2_topk3: 196 scored (F1=0.208)\n",
      "  OK bench_gpt4o-mini_gold_O1_L2_topk3: 121 scored (F1=0.169)\n",
      "  OK bench_gpt4o-mini_gold_O2_L2_topk3: 92 scored (F1=0.137)\n",
      "  OK bench_gpt4o-mini_gold_O3_L2_topk3: 85 scored (F1=0.158)\n",
      "  OK bench_gpt4o-mini_silver_O0_L2_topk3: 29 scored (F1=0.200)\n",
      "  OK bench_gpt4o-mini_silver_O1_L2_topk3: 23 scored (F1=0.325)\n",
      "  OK bench_gpt4o-mini_silver_O2_L2_topk3: 32 scored (F1=0.232)\n",
      "  OK bench_gpt4o-mini_silver_O3_L2_topk3: 34 scored (F1=0.184)\n",
      "  OK bench_gpt4o-mini_bronze_O1_L2_topk3: 17 scored (F1=0.161)\n",
      "  OK bench_gpt4o-mini_bronze_O2_L2_topk3: 30 scored (F1=0.098)\n",
      "  OK bench_gpt4o-mini_bronze_O3_L2_topk3: 33 scored (F1=0.089)\n",
      "  OK bench_deepseek-v3_gold_O0_L2_topk3: 196 scored (F1=0.241)\n",
      "  OK bench_deepseek-v3_gold_O1_L2_topk3: 121 scored (F1=0.223)\n",
      "  OK bench_deepseek-v3_gold_O2_L2_topk3: 92 scored (F1=0.182)\n",
      "  OK bench_deepseek-v3_gold_O3_L2_topk3: 85 scored (F1=0.184)\n",
      "  OK bench_deepseek-v3_silver_O0_L2_topk3: 29 scored (F1=0.391)\n",
      "  OK bench_deepseek-v3_silver_O1_L2_topk3: 23 scored (F1=0.388)\n",
      "  OK bench_deepseek-v3_silver_O2_L2_topk3: 32 scored (F1=0.299)\n",
      "  OK bench_deepseek-v3_silver_O3_L2_topk3: 34 scored (F1=0.297)\n",
      "  OK bench_deepseek-v3_bronze_O1_L2_topk3: 17 scored (F1=0.188)\n",
      "  OK bench_deepseek-v3_bronze_O2_L2_topk3: 30 scored (F1=0.129)\n",
      "  OK bench_deepseek-v3_bronze_O3_L2_topk3: 33 scored (F1=0.105)\n",
      "  OK bench_gpt51_gold_O0_L2_topk3: 196 scored (F1=0.276)\n",
      "  OK bench_gpt51_gold_O1_L2_topk3: 121 scored (F1=0.281)\n",
      "  OK bench_gpt51_gold_O2_L2_topk3: 92 scored (F1=0.241)\n",
      "  OK bench_gpt51_gold_O3_L2_topk3: 85 scored (F1=0.286)\n",
      "  OK bench_gpt51_silver_O0_L2_topk3: 29 scored (F1=0.287)\n",
      "  OK bench_gpt51_silver_O1_L2_topk3: 23 scored (F1=0.318)\n",
      "  OK bench_gpt51_silver_O2_L2_topk3: 32 scored (F1=0.289)\n",
      "  OK bench_gpt51_silver_O3_L2_topk3: 34 scored (F1=0.345)\n",
      "  OK bench_gpt51_bronze_O1_L2_topk3: 17 scored (F1=0.214)\n",
      "  OK bench_gpt51_bronze_O2_L2_topk3: 30 scored (F1=0.146)\n",
      "  OK bench_gpt51_bronze_O3_L2_topk3: 33 scored (F1=0.140)\n",
      "  OK bench_claude-sonnet45_gold_O0_L2_topk3: 196 scored (F1=0.287)\n",
      "  OK bench_claude-sonnet45_gold_O1_L2_topk3: 121 scored (F1=0.300)\n",
      "  OK bench_claude-sonnet45_gold_O2_L2_topk3: 92 scored (F1=0.297)\n",
      "  OK bench_claude-sonnet45_gold_O3_L2_topk3: 85 scored (F1=0.319)\n",
      "  OK bench_claude-sonnet45_silver_O0_L2_topk3: 29 scored (F1=0.360)\n",
      "  OK bench_claude-sonnet45_silver_O1_L2_topk3: 23 scored (F1=0.413)\n",
      "  OK bench_claude-sonnet45_silver_O2_L2_topk3: 32 scored (F1=0.338)\n",
      "  OK bench_claude-sonnet45_silver_O3_L2_topk3: 34 scored (F1=0.328)\n",
      "  OK bench_claude-sonnet45_bronze_O1_L2_topk3: 17 scored (F1=0.255)\n",
      "  OK bench_claude-sonnet45_bronze_O2_L2_topk3: 30 scored (F1=0.154)\n",
      "  OK bench_claude-sonnet45_bronze_O3_L2_topk3: 33 scored (F1=0.120)\n",
      "  OK bench_llama31-70b_gold_O0_L2_topk3: 196 scored (F1=0.242)\n",
      "  OK bench_llama31-70b_gold_O1_L2_topk3: 121 scored (F1=0.221)\n",
      "  OK bench_llama31-70b_gold_O2_L2_topk3: 92 scored (F1=0.181)\n",
      "  OK bench_llama31-70b_gold_O3_L2_topk3: 85 scored (F1=0.204)\n",
      "  OK bench_llama31-70b_silver_O0_L2_topk3: 29 scored (F1=0.245)\n",
      "  OK bench_llama31-70b_silver_O1_L2_topk3: 23 scored (F1=0.388)\n",
      "  OK bench_llama31-70b_silver_O2_L2_topk3: 32 scored (F1=0.303)\n",
      "  OK bench_llama31-70b_silver_O3_L2_topk3: 34 scored (F1=0.262)\n",
      "  OK bench_llama31-70b_bronze_O1_L2_topk3: 17 scored (F1=0.200)\n",
      "  OK bench_llama31-70b_bronze_O2_L2_topk3: 30 scored (F1=0.127)\n",
      "  OK bench_llama31-70b_bronze_O3_L2_topk3: 33 scored (F1=0.103)\n",
      "  OK bench_deepseek-r1_gold_O0_L2_topk3: 196 scored (F1=0.275)\n",
      "  OK bench_deepseek-r1_gold_O1_L2_topk3: 119 scored (F1=0.258)\n",
      "  OK bench_deepseek-r1_gold_O2_L2_topk3: 88 scored (F1=0.250)\n",
      "  OK bench_deepseek-r1_gold_O3_L2_topk3: 84 scored (F1=0.247)\n",
      "  OK bench_deepseek-r1_silver_O0_L2_topk3: 29 scored (F1=0.377)\n",
      "  OK bench_deepseek-r1_silver_O1_L2_topk3: 23 scored (F1=0.405)\n",
      "  OK bench_deepseek-r1_silver_O2_L2_topk3: 32 scored (F1=0.264)\n",
      "  OK bench_deepseek-r1_silver_O3_L2_topk3: 34 scored (F1=0.319)\n",
      "  OK bench_deepseek-r1_bronze_O1_L2_topk3: 17 scored (F1=0.227)\n",
      "  OK bench_deepseek-r1_bronze_O2_L2_topk3: 30 scored (F1=0.180)\n",
      "  OK bench_deepseek-r1_bronze_O3_L2_topk3: 33 scored (F1=0.141)\n",
      "  OK bench_qwen3-coder_gold_O0_L2_topk3: 196 scored (F1=0.256)\n",
      "  OK bench_qwen3-coder_gold_O1_L2_topk3: 121 scored (F1=0.242)\n",
      "  OK bench_qwen3-coder_gold_O2_L2_topk3: 92 scored (F1=0.226)\n",
      "  OK bench_qwen3-coder_gold_O3_L2_topk3: 85 scored (F1=0.224)\n",
      "  OK bench_qwen3-coder_silver_O0_L2_topk3: 29 scored (F1=0.282)\n",
      "  OK bench_qwen3-coder_silver_O1_L2_topk3: 23 scored (F1=0.393)\n",
      "  OK bench_qwen3-coder_silver_O2_L2_topk3: 32 scored (F1=0.356)\n",
      "  OK bench_qwen3-coder_silver_O3_L2_topk3: 34 scored (F1=0.348)\n",
      "  OK bench_qwen3-coder_bronze_O1_L2_topk3: 17 scored (F1=0.179)\n",
      "  OK bench_qwen3-coder_bronze_O2_L2_topk3: 30 scored (F1=0.130)\n",
      "  OK bench_qwen3-coder_bronze_O3_L2_topk3: 33 scored (F1=0.108)\n",
      "\n",
      "Scored: 77, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "all_exps = api(\"/data/experiments\")\n",
    "benchmark_exps = [e for e in all_exps if 'benchmark-v2' in e.get('tags', [])]\n",
    "\n",
    "scored = 0\n",
    "score_errors = 0\n",
    "for e in benchmark_exps:\n",
    "    exp_id = e['id']\n",
    "    try:\n",
    "        results = api(f\"/results/{exp_id}\")\n",
    "        if not results or not results.get('rows'):\n",
    "            continue\n",
    "        \n",
    "        resp = requests.post(f\"{API}/results/{exp_id}/score\")\n",
    "        if resp.status_code == 200:\n",
    "            score_data = resp.json()\n",
    "            n = score_data.get('scored', 0)\n",
    "            f1 = score_data.get('mean_token_f1', 0)\n",
    "            print(f\"  OK {exp_id}: {n} scored (F1={f1:.3f})\")\n",
    "            scored += 1\n",
    "        else:\n",
    "            print(f\"  WARN {exp_id}: {resp.status_code}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  ERR {exp_id}: {exc}\")\n",
    "        score_errors += 1\n",
    "\n",
    "print(f\"\\nScored: {scored}, Errors: {score_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a67dde",
   "metadata": {},
   "source": [
    "## §8 — Progress Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922c0045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark progress (36 experiments):\n",
      "  No results yet: 3\n",
      "  Has results:    33\n",
      "  Scored:         33\n",
      "\n",
      "Per-model progress:\n",
      "  deepseek/deepseek-chat-v3-0324           [##################..]  11/ 12 (92%)\n",
      "  openai/gpt-4o-mini                       [##################..]  11/ 12 (92%)\n",
      "  openai/gpt-5.1                           [##################..]  11/ 12 (92%)\n"
     ]
    }
   ],
   "source": [
    "all_exps = api(\"/data/experiments\")\n",
    "benchmark_exps = [e for e in all_exps if 'benchmark-v2' in e.get('tags', [])]\n",
    "\n",
    "progress = {\"no_results\": 0, \"has_results\": 0, \"scored\": 0}\n",
    "model_progress = {}\n",
    "\n",
    "for e in benchmark_exps:\n",
    "    exp_id = e['id']\n",
    "    model = e['model']\n",
    "    if model not in model_progress:\n",
    "        model_progress[model] = {\"total\": 0, \"done\": 0}\n",
    "    model_progress[model][\"total\"] += 1\n",
    "    \n",
    "    try:\n",
    "        results = api(f\"/results/{exp_id}\")\n",
    "        if results and results.get('rows'):\n",
    "            progress[\"has_results\"] += 1\n",
    "            model_progress[model][\"done\"] += 1\n",
    "            try:\n",
    "                resp = api(f\"/results/{exp_id}/scores\")\n",
    "                rows = resp.get(\"rows\", []) if isinstance(resp, dict) else resp\n",
    "                if rows and any(s.get('token_f1') is not None for s in rows):\n",
    "                    progress[\"scored\"] += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            progress[\"no_results\"] += 1\n",
    "    except Exception:\n",
    "        progress[\"no_results\"] += 1\n",
    "\n",
    "print(f\"Benchmark progress ({len(benchmark_exps)} experiments):\")\n",
    "print(f\"  No results yet: {progress['no_results']}\")\n",
    "print(f\"  Has results:    {progress['has_results']}\")\n",
    "print(f\"  Scored:         {progress['scored']}\")\n",
    "print()\n",
    "print(\"Per-model progress:\")\n",
    "for m in sorted(model_progress.keys()):\n",
    "    p = model_progress[m]\n",
    "    pct = (p['done'] / p['total'] * 100) if p['total'] else 0\n",
    "    done_blocks = int(pct // 5)\n",
    "    bar = '#' * done_blocks + '.' * (20 - done_blocks)\n",
    "    print(f\"  {m:40s} [{bar}] {p['done']:3d}/{p['total']:3d} ({pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be28c63",
   "metadata": {},
   "source": [
    "## §9 — Leak-Proof Assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.llm_contract import validate_no_leakage, FORBIDDEN_KEYS\n",
    "\n",
    "sample_fns = api(\"/llm/functions\", params={\"opt\": \"O0\", \"tier\": \"GOLD\", \"limit\": 50, \"context_level\": \"L2\"})\n",
    "\n",
    "violations = 0\n",
    "for fn in sample_fns:\n",
    "    leaked = validate_no_leakage(fn)\n",
    "    if leaked:\n",
    "        print(f\"  LEAK: {fn.get('dwarf_function_id', '?')}: {leaked}\")\n",
    "        violations += 1\n",
    "\n",
    "if violations == 0:\n",
    "    print(f\"LEAK-PROOF: {len(sample_fns)} functions checked at L2 context - zero GT fields\")\n",
    "    print(f\"   Forbidden keys blocked: {len(FORBIDDEN_KEYS)}\")\n",
    "else:\n",
    "    print(f\"!! {violations} VIOLATIONS DETECTED - fix immediately\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561da31",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference\n",
    "\n",
    "### CLI execution (alternative to notebook)\n",
    "```bash\n",
    "cd reforge\n",
    "python -m workers.llm.runner --experiment bench_gpt4o-mini_gold_O0_L2_topk3 --api-base http://localhost:8080 --concurrency 5\n",
    "python -m workers.llm.runner --experiment bench_gpt4o-mini_gold_O0_L2_topk3 --api-base http://localhost:8080 --dry-run\n",
    "```\n",
    "\n",
    "### Analysis\n",
    "See `analysis.ipynb` for all figures, tables, and statistical tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
